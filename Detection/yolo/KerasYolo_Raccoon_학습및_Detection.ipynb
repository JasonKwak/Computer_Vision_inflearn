{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raccoon 데이터 세트를 YOLO V3로 학습한 뒤 학습모델을 이용하여 이미지와 비디오에 Object Detection 적용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raccoon 데이터 세트 download\n",
    "* Racoon 데이터 세트를 git clone으로 복사합니다. git clone https://github.com/experiencor/raccoon_dataset.git\n",
    "* 이미지와 annoation 디렉토리를 제외하고 모두 삭제합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayoon.kim000/DLCV/data/raccoon/annotations\n",
      "파일 개수는: 207\n",
      "['raccoon-173.xml', 'raccoon-171.xml', 'raccoon-106.xml', 'raccoon-100.xml', 'raccoon-16.xml', 'raccoon-75.xml', 'raccoon_anno_tensor_train.csv', 'raccoon-142.xml', 'raccoon-116.xml', 'raccoon-59.xml', 'raccoon-118.xml', 'raccoon-31.xml', 'raccoon-187.xml', 'raccoon-128.xml', 'raccoon-72.xml', 'raccoon-65.xml', 'raccoon-62.xml', 'raccoon-8.xml', 'raccoon-17.xml', 'raccoon-132.xml', 'raccoon-166.xml', 'raccoon-113.xml', 'raccoon-146.xml', 'raccoon-25.xml', 'raccoon-125.xml', 'raccoon-150.xml', 'raccoon-84.xml', 'raccoon-137.xml', 'raccoon_anno_tensor_valid.csv', 'raccoon_anno.csv', 'raccoon-191.xml', 'raccoon-91.xml', 'raccoon-88.xml', 'raccoon-158.xml', 'raccoon-4.xml', 'raccoon-5.xml', 'raccoon-136.xml', 'raccoon-43.xml', 'raccoon-103.xml', 'raccoon-33.xml', 'raccoon-157.xml', 'raccoon-79.xml', 'raccoon-9.xml', 'raccoon-184.xml', 'raccoon-122.xml', 'raccoon-172.xml', 'raccoon-45.xml', 'raccoon-178.xml', 'raccoon-13.xml', 'raccoon-69.xml', 'raccoon-57.xml', 'raccoon-83.xml', 'raccoon-198.xml', 'raccoon-123.xml', 'raccoon-169.xml', 'raccoon-107.xml', 'raccoon-27.xml', 'raccoon-18.xml', 'raccoon-188.xml', 'raccoon-60.xml', 'raccoon-70.xml', 'raccoon-145.xml', 'raccoon-10.xml', 'raccoon-156.xml', 'raccoon-111.xml', 'raccoon_anno_retina_valid.csv', 'raccoon-73.xml', 'raccoon-160.xml', 'raccoon-80.xml', 'raccoon-133.xml', 'raccoon-48.xml', 'raccoon-11.xml', 'raccoon-101.xml', 'raccoon-134.xml', 'raccoon-14.xml', 'raccoon-81.xml', 'raccoon-22.xml', 'raccoon-135.xml', 'raccoon-90.xml', 'raccoon-37.xml', 'raccoon-120.xml', 'raccoon-76.xml', 'raccoon-32.xml', 'raccoon-66.xml', 'raccoon-153.xml', 'raccoon-183.xml', 'raccoon-42.xml', 'raccoon-94.xml', 'raccoon_anno_retina.csv', 'raccoon-50.xml', 'raccoon-98.xml', 'raccoon-144.xml', 'raccoon-168.xml', 'raccoon-38.xml', 'raccoon-1.xml', 'raccoon-199.xml', 'raccoon-39.xml', 'raccoon-129.xml', 'raccoon-114.xml', 'raccoon-89.xml', 'raccoon_class.txt', 'raccoon-181.xml', 'raccoon-7.xml', 'raccoon-159.xml', 'raccoon-162.xml', 'raccoon-87.xml', 'raccoon-154.xml', 'raccoon-95.xml', 'raccoon-52.xml', 'raccoon-165.xml', 'raccoon-51.xml', 'raccoon-96.xml', 'raccoon-115.xml', 'raccoon-126.xml', 'raccoon-186.xml', 'raccoon-196.xml', 'raccoon-3.xml', 'raccoon-23.xml', 'raccoon-40.xml', 'raccoon-24.xml', 'raccoon-104.xml', 'raccoon-71.xml', 'raccoon-121.xml', 'raccoon-192.xml', 'raccoon-197.xml', 'raccoon-130.xml', 'raccoon-20.xml', 'raccoon-44.xml', 'raccoon-102.xml', 'raccoon-164.xml', 'raccoon-175.xml', 'raccoon-180.xml', 'raccoon-35.xml', 'raccoon-148.xml', 'raccoon-56.xml', 'raccoon-195.xml', 'raccoon-74.xml', 'raccoon-190.xml', 'raccoon-152.xml', 'raccoon-49.xml', 'raccoon-110.xml', 'raccoon-2.xml', 'raccoon-82.xml', 'raccoon-64.xml', 'raccoon-155.xml', 'raccoon-30.xml', 'raccoon-105.xml', 'raccoon-174.xml', 'raccoon-41.xml', 'raccoon-170.xml', 'raccoon-54.xml', 'raccoon_anno_retina_train.csv', 'raccoon-29.xml', 'raccoon-109.xml', 'raccoon-53.xml', 'raccoon-58.xml', 'raccoon-143.xml', 'raccoon-28.xml', 'raccoon-46.xml', 'raccoon-26.xml', 'raccoon-19.xml', 'raccoon-99.xml', 'raccoon-15.xml', 'raccoon-161.xml', 'raccoon-141.xml', 'raccoon-167.xml', 'raccoon-112.xml', 'raccoon-61.xml', 'raccoon-163.xml', 'raccoon-6.xml', 'raccoon-147.xml', 'raccoon-138.xml', 'raccoon-36.xml', 'raccoon-78.xml', 'raccoon-131.xml', 'raccoon-124.xml', 'raccoon-68.xml', 'raccoon-200.xml', 'raccoon-177.xml', 'raccoon-108.xml', 'raccoon-92.xml', 'raccoon-85.xml', 'raccoon-140.xml', 'raccoon-179.xml', 'raccoon-47.xml', 'raccoon-34.xml', 'raccoon-117.xml', 'raccoon-182.xml', 'raccoon-127.xml', 'raccoon-77.xml', 'raccoon-97.xml', 'raccoon-193.xml', 'raccoon-21.xml', 'raccoon-119.xml', 'raccoon-176.xml', 'raccoon-55.xml', 'raccoon-86.xml', 'raccoon-93.xml', 'raccoon-149.xml', 'raccoon-67.xml', 'raccoon-151.xml', 'raccoon-194.xml', 'raccoon-185.xml', 'raccoon-12.xml', 'raccoon-63.xml', 'raccoon-139.xml', 'raccoon-189.xml']\n"
     ]
    }
   ],
   "source": [
    "# annotation과 image 디렉토리 설정. annotation디렉토리에 있는 파일 확인. \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HOME_DIR = str(Path.home())\n",
    "\n",
    "ANNO_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/annotations')\n",
    "IMAGE_DIR = os.path.join(HOME_DIR, 'DLCV/data/raccoon/images')\n",
    "print(ANNO_DIR)\n",
    "\n",
    "files = os.listdir(ANNO_DIR)\n",
    "print('파일 개수는:',len(files))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_csv(path, output_filename):\n",
    "    xml_list = []\n",
    "    # xml 확장자를 가진 모든 파일의 절대 경로로 xml_file할당. \n",
    "    with open(output_filename, \"w\") as train_csv_file:\n",
    "        for xml_file in glob.glob(path + '/*.xml'):\n",
    "            # xml 파일을 parsing하여 XML Element형태의 Element Tree를 생성하여 object 정보를 추출. \n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            # 파일내에 있는 모든 object Element를 찾음. \n",
    "            full_image_name = os.path.join(IMAGE_DIR, root.find('filename').text)\n",
    "            value_str_list = ' '\n",
    "            for obj in root.findall('object'):\n",
    "                \n",
    "                xmlbox = obj.find('bndbox')\n",
    "                x1 = int(xmlbox.find('xmin').text)\n",
    "                y1 = int(xmlbox.find('ymin').text)\n",
    "                x2 = int(xmlbox.find('xmax').text)\n",
    "                y2 = int(xmlbox.find('ymax').text)\n",
    "                # 단 하나의 class_id raccoon\n",
    "                class_id = 0\n",
    "                value_str = ('{0},{1},{2},{3},{4}').format(x1, y1, x2, y2, class_id)\n",
    "                value_str_list = value_str_list+value_str_list+' ' \n",
    "                # object별 정보를 tuple형태로 object_list에 저장. \n",
    "            train_csv_file.write(full_image_name+' '+ value_str+'\\n')\n",
    "        # xml file 찾는 for loop 종료 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayoon.kim000/DLCV/data/raccoon/annotations/raccoon_anno.csv\n"
     ]
    }
   ],
   "source": [
    "xml_to_csv(ANNO_DIR, os.path.join(ANNO_DIR,'raccoon_anno.csv'))\n",
    "print(os.path.join(ANNO_DIR,'raccoon_anno.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "LOCAL_PACKAGE_DIR = os.path.abspath(\"./keras-yolo3\")\n",
    "sys.path.append(LOCAL_PACKAGE_DIR)\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Create YOLOv3 model with 9 anchors and 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 18) vs (255, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 18) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 18) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/home/sayoon.kim000/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/saving.py:1008: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights /home/sayoon.kim000/DLCV/Detection/yolo/keras-yolo3/model_data/yolo.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "Train on 180 samples, val on 20 samples, with batch size 4.\n",
      "Epoch 1/50\n",
      "45/45 [==============================] - 14s 304ms/step - loss: 1433.4340 - val_loss: 190.6890\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 10s 213ms/step - loss: 152.0204 - val_loss: 94.8607\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 11s 236ms/step - loss: 89.6848 - val_loss: 64.6404\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 9s 200ms/step - loss: 66.9547 - val_loss: 49.9326\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 11s 241ms/step - loss: 52.0843 - val_loss: 39.3436\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 11s 238ms/step - loss: 45.0872 - val_loss: 33.6091\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 11s 234ms/step - loss: 37.6555 - val_loss: 31.7680\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 11s 239ms/step - loss: 34.1296 - val_loss: 27.8850\n",
      "Epoch 9/50\n",
      " 2/45 [>.............................] - ETA: 3s - loss: 32.0265"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0e1a868a4ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             callbacks=[logging, checkpoint])\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'trained_weights_stage_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf113/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import get_classes, get_anchors\n",
    "from train import create_model, data_generator, data_generator_wrapper\n",
    "\n",
    "BASE_DIR = os.path.join(HOME_DIR, 'DLCV/Detection/yolo/keras-yolo3')\n",
    "\n",
    "## 학습을 위한 기반 환경 설정. annotation 파일 위치, epochs시 저장된 모델 파일, Object클래스 파일, anchor 파일.\n",
    "annotation_path = os.path.join(ANNO_DIR, 'raccoon_anno.csv')\n",
    "log_dir = os.path.join(BASE_DIR, 'snapshots/000/')\n",
    "classes_path = os.path.join(BASE_DIR, 'model_data/raccoon_class.txt')\n",
    "anchors_path = os.path.join(BASE_DIR,'model_data/yolo_anchors.txt')\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "# 아래는 원본 train.py에서 weights_path 변경을 위해 임의 수정. 최초 weight 모델 로딩은 coco로 pretrained된 모델 로딩. \n",
    "# tiny yolo로 모델을 학습 원할 시 아래를 tiny-yolo.h5로 수정. \n",
    "model_weights_path = os.path.join(BASE_DIR, 'model_data/yolo.h5' )\n",
    "\n",
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "is_tiny_version = len(anchors)==6 # default setting\n",
    "# create_tiny_model(), create_model() 함수의 인자 설정을 원본 train.py에서 수정. \n",
    "if is_tiny_version:\n",
    "    model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path=model_weights_path)\n",
    "else:\n",
    "    model = create_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path=model_weights_path) # make sure you know what you freeze\n",
    "\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "val_split = 0.1\n",
    "\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "# Train with frozen layers first, to get a stable loss.\n",
    "# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "if True:\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "        # use custom yolo_loss Lambda layer.\n",
    "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "    batch_size = 4\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[logging, checkpoint])\n",
    "    model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "# Unfreeze and continue training, to fine-tune.\n",
    "# Train longer if the result is not good.\n",
    "if True:\n",
    "    for i in range(len(model.layers)):\n",
    "        model.layers[i].trainable = True\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "    print('Unfreeze all of the layers.')\n",
    "\n",
    "    batch_size = 4 # note that more GPU memory is required after unfreezing the body\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "        steps_per_epoch=max(1, num_train//batch_size),\n",
    "        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "        validation_steps=max(1, num_val//batch_size),\n",
    "        epochs=100,\n",
    "        initial_epoch=50,\n",
    "        callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "    model.save_weights(log_dir + 'trained_weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 객체 생성. \n",
    "import sys\n",
    "import argparse\n",
    "from yolo import YOLO, detect_video\n",
    "#keras-yolo에서 image처리를 주요 PIL로 수행. \n",
    "from PIL import Image\n",
    "\n",
    "LOCAL_PACKAGE_DIR = os.path.abspath(\"./keras-yolo3\")\n",
    "sys.path.append(LOCAL_PACKAGE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raccoon_yolo = YOLO(model_path=os.path.join(HOME_DIR,'DLCV/Detection/yolo/keras-yolo3/snapshots/000/trained_weights_final.h5'),\n",
    "            anchors_path='~/DLCV/Detection/yolo/keras-yolo3/model_data/yolo_anchors.txt',\n",
    "            classes_path='~/DLCV/Detection/yolo/keras-yolo3/model_data/raccoon_class.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(os.path.join(IMAGE_DIR, 'raccoon-171.jpg'))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_img = raccoon_yolo.detect_image(img)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(detected_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임의의 16개의 원본 이미지를 추출하여 Object Detected된 결과 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 모든 이미지 파일중에서 임의의 16개 파일만 설정. \n",
    "all_image_files = glob.glob(IMAGE_DIR + '/*.jpg')\n",
    "all_image_files = np.array(all_image_files)\n",
    "file_cnt = all_image_files.shape[0]\n",
    "show_cnt = 16\n",
    "\n",
    "show_indexes = np.random.choice(file_cnt, show_cnt)\n",
    "show_files = all_image_files[show_indexes]\n",
    "print(show_files)\n",
    "fig, axs = plt.subplots(figsize=(24,24) , ncols=4 , nrows=4)\n",
    "\n",
    "for i , filename in enumerate(show_files):\n",
    "    print(filename)\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    img = Image.open(os.path.join(IMAGE_DIR, filename))\n",
    "    detected_image = raccoon_yolo.detect_image(img)\n",
    "    axs[row][col].imshow(detected_image)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Object Detection 수행. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "def detect_video_yolo(model, input_path, output_path=\"\"):\n",
    "    \n",
    "    start = time.time()\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    #codec = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    vid_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n",
    "    \n",
    "    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print('총 Frame 갯수:', frame_cnt, '원본 영상 FPS:',vid_fps, '원본 Frame 크기:', vid_size)\n",
    "    index = 0\n",
    "    while True:\n",
    "        hasFrame, image_frame = cap.read()\n",
    "        if not hasFrame:\n",
    "            print('프레임이 없거나 종료 되었습니다.')\n",
    "            break\n",
    "        start = time.time()\n",
    "        # PIL Package를 내부에서 사용하므로 cv2에서 읽은 image_frame array를 다시 PIL의 Image형태로 변환해야 함.  \n",
    "        image = Image.fromarray(image_frame)\n",
    "        # 아래는 인자로 입력된 yolo객체의 detect_image()로 변환한다.\n",
    "        detected_image = model.detect_image(image)\n",
    "        # cv2의 video writer로 출력하기 위해 다시 PIL의 Image형태를 array형태로 변환 \n",
    "        result = np.asarray(detected_image)\n",
    "        index +=1\n",
    "        print('#### frame:{0} 이미지 처리시간:{1}'.format(index, round(time.time()-start,3)))\n",
    "        \n",
    "        vid_writer.write(result)\n",
    "    \n",
    "    vid_writer.release()\n",
    "    cap.release()\n",
    "    print('### Video Detect 총 수행시간:', round(time.time()-start, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_video_yolo(raccoon_yolo, '../../data/video/jack_and_raccoon.mp4', '../../data/output/jack_and_raccoon_yolo_01.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tiny yolo로 학습. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from train import get_classes, get_anchors\n",
    "from train import create_model, create_tiny_model, data_generator, data_generator_wrapper\n",
    "\n",
    "def train_yolo(pretrained_path, annotation_path,classes_path, anchors_path,log_dir,trained_model_name, b_size, epochs_cnt):      \n",
    "        \n",
    "        print('pretrained_path:', pretrained_path)\n",
    "        class_names = get_classes(classes_path)\n",
    "        num_classes = len(class_names)\n",
    "        anchors = get_anchors(anchors_path)\n",
    "\n",
    "        input_shape = (416,416) # multiple of 32, hw\n",
    "        # tiny yolo여부를 anchor 설정 파일에서 자동으로 알 수 있음. anchor갯수가 6개이면 tiny yolo\n",
    "        is_tiny_version = len(anchors)==6 # default setting\n",
    "        \n",
    "        # create_tiny_model(), create_model() 함수의 인자 설정을 원본 train.py에서 수정.\n",
    "        if is_tiny_version:\n",
    "            model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "                freeze_body=2, weights_path=pretrained_path)\n",
    "        else:\n",
    "            model = create_model(input_shape, anchors, num_classes,\n",
    "                freeze_body=2, weights_path=pretrained_path) # make sure you know what you freeze\n",
    "\n",
    "        logging = TensorBoard(log_dir=log_dir)\n",
    "        checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "            monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "        val_split = 0.1\n",
    "        with open(annotation_path) as f:\n",
    "            lines = f.readlines()\n",
    "        np.random.seed(10101)\n",
    "        np.random.shuffle(lines)\n",
    "        np.random.seed(None)\n",
    "        num_val = int(len(lines)*val_split)\n",
    "        num_train = len(lines) - num_val\n",
    "\n",
    "        # Train with frozen layers first, to get a stable loss.\n",
    "        # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "        if True:\n",
    "            model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "                # use custom yolo_loss Lambda layer.\n",
    "                'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "            batch_size = b_size\n",
    "            print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "            model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                    steps_per_epoch=max(1, num_train//batch_size),\n",
    "                    validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                    validation_steps=max(1, num_val//batch_size),\n",
    "                    epochs=int(epochs_cnt/2),\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=[logging, checkpoint])\n",
    "            model.save_weights(log_dir + trained_model_name+'_stage_1.h5')\n",
    "\n",
    "        # Unfreeze and continue training, to fine-tune.\n",
    "        # Train longer if the result is not good.\n",
    "        if True:\n",
    "            for i in range(len(model.layers)):\n",
    "                model.layers[i].trainable = True\n",
    "            model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "            print('Unfreeze all of the layers.')\n",
    "\n",
    "            batch_size = b_size # note that more GPU memory is required after unfreezing the body\n",
    "            print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "            model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=epochs_cnt,\n",
    "                initial_epoch=int(epochs_cnt/2),\n",
    "                callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "            model.save_weights(log_dir + trained_model_name+'_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(HOME_DIR, 'DLCV/Detection/yolo/keras-yolo3')\n",
    "# keras-yolo3에서 convert 된 yolo-tiny pretrained 모델을 사용해야 함. \n",
    "pretrained_path = os.path.join(BASE_DIR, 'model_data/yolo-tiny.h5')\n",
    "annotation_path = os.path.join(ANNO_DIR,'raccoon_anno.csv')\n",
    "classes_path = os.path.join(BASE_DIR, 'model_data/raccoon_class.txt')\n",
    "anchors_path = os.path.join(BASE_DIR, 'model_data/tiny_yolo_anchors.txt')\n",
    "log_dir = os.path.join(BASE_DIR,'snapshots/000/')\n",
    "trained_model_name = 'raccoon'\n",
    "b_size=4\n",
    "epochs_cnt = 100\n",
    "\n",
    "train_yolo(pretrained_path, annotation_path,classes_path, anchors_path, log_dir,trained_model_name, b_size, epochs_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raccoon_tiny_yolo = YOLO(model_path=os.path.join(HOME_DIR,'DLCV/Detection/yolo/keras-yolo3/snapshots/000/raccoon_final.h5'),\n",
    "            anchors_path='~/DLCV/Detection/yolo/keras-yolo3/model_data/tiny_yolo_anchors.txt',\n",
    "            classes_path='~/DLCV/Detection/yolo/keras-yolo3/model_data/raccoon_class.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 모든 이미지 파일중에서 임의의 16개 파일만 설정. \n",
    "all_image_files = glob.glob(IMAGE_DIR + '/*.jpg')\n",
    "all_image_files = np.array(all_image_files)\n",
    "file_cnt = all_image_files.shape[0]\n",
    "show_cnt = 16\n",
    "\n",
    "show_indexes = np.random.choice(file_cnt, show_cnt)\n",
    "show_files = all_image_files[show_indexes]\n",
    "print(show_files)\n",
    "fig, axs = plt.subplots(figsize=(24,24) , ncols=4 , nrows=4)\n",
    "\n",
    "for i , filename in enumerate(show_files):\n",
    "    print(filename)\n",
    "    row = int(i/4)\n",
    "    col = i%4\n",
    "    img = Image.open(os.path.join(IMAGE_DIR, filename))\n",
    "    detected_image = raccoon_tiny_yolo.detect_image(img)\n",
    "    axs[row][col].imshow(detected_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_video_yolo(raccoon_tiny_yolo, '../../data/video/jack_and_raccoon.mp4', '../../data/output/jack_and_raccoon_tiny_yolo_01.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf113",
   "language": "python",
   "name": "tf113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
